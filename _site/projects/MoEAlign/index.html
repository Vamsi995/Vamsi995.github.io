<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>MoE Align: Quantifying Expert Similarity Across Transformer Layers | Alisetti Sai Vamsi</title>
<meta name="description" content="Modern Mixture-of-Experts (MoE) transformers activate a subset of expert functions per token, enabling large-scale models with efficient inference. These expert layers are usually assumed to be depth-specific and non-interchangeable. This project explores whether experts across layers perform functionally similar transformations, making cross-layer expert reuse feasible.">


  <meta name="author" content="Alisetti Sai Vamsi">
  
  <meta property="article:author" content="Alisetti Sai Vamsi">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Alisetti Sai Vamsi">
<meta property="og:title" content="MoE Align: Quantifying Expert Similarity Across Transformer Layers">
<meta property="og:url" content="http://localhost:4000/projects/MoEAlign/">


  <meta property="og:description" content="Modern Mixture-of-Experts (MoE) transformers activate a subset of expert functions per token, enabling large-scale models with efficient inference. These expert layers are usually assumed to be depth-specific and non-interchangeable. This project explores whether experts across layers perform functionally similar transformations, making cross-layer expert reuse feasible.">



  <meta property="og:image" content="http://localhost:4000/assets/images/projects/MoEAlign/activations.png">





  <meta property="article:published_time" content="2025-07-04T17:29:43-07:00">





  

  


<link rel="canonical" href="http://localhost:4000/projects/MoEAlign/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Alisetti Sai Vamsi",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->




<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/landing/logo2.png" alt="Alisetti Sai Vamsi"></a>
        
        <a class="site-title" href="/">
          Alisetti Sai Vamsi
          <span class="site-subtitle">Portfolio</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/research/">Research</a>
            </li><li class="masthead__menu-item">
              <a href="/projects/">Projects</a>
            </li><li class="masthead__menu-item">
              <a href="/education/">Education</a>
            </li><li class="masthead__menu-item">
              <a href="/achievements/">Achievements</a>
            </li><li class="masthead__menu-item">
              <a href="/assets/CV/Resume.pdf">CV</a>
            </li><li class="masthead__menu-item">
              <a href="/misc/">Misc</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      




  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/" itemprop="item"><span itemprop="name">Home</span></a>

          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">></span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/projects" itemprop="item"><span itemprop="name">Projects</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">></span>
      
    
      
      
        <li class="current">MoE Align: Quantifying Expert Similarity Across Transformer Layers</li>
      
    
  </ol>
</nav>

  


<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true},
      jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
      extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
      TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
      equationNumbers: {
      autoNumber: "AMS"
      }
    }
  });
</script>

<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      <h3>GitHub</h3>
      <p><a href="https://github.com/Vamsi995/exsim">https://github.com/Vamsi995/exsim</a></p>

      
    
      
      <h3>Technology Frameworks & Languages</h3>
      <p>Python, PyTorch, Co-lab Notebooks, Streamlit, Flask, Docker</p>

      
    
    
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="MoE Align: Quantifying Expert Similarity Across Transformer Layers">
    <meta itemprop="description" content="Modern Mixture-of-Experts (MoE) transformers activate a subset of expert functions per token, enabling large-scale models with efficient inference. These expert layers are usually assumed to be depth-specific and non-interchangeable. This project explores whether experts across layers perform functionally similar transformations, making cross-layer expert reuse feasible.">
    <meta itemprop="datePublished" content="2025-07-04T17:29:43-07:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/projects/MoEAlign/" class="u-url" itemprop="url">MoE Align: Quantifying Expert Similarity Across Transformer Layers
</a>
          </h1>
          


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#-overview">🧠 Overview</a></li><li><a href="#-core-idea">🔍 Core Idea</a></li><li><a href="#-problem-formulation">🧪 Problem Formulation</a></li><li><a href="#️-experimental-setup">⚙️ Experimental Setup</a></li><li><a href="#-results">📊 Results</a><ul><li><a href="#-cross-layer-expert-alignment">🔁 Cross-Layer Expert Alignment</a></li></ul></li><li><a href="#-additional-experiment-layer-swapping">🧩 Additional Experiment: Layer Swapping</a></li><li><a href="#-future-work">🔮 Future Work</a></li><li><a href="#-authors">👥 Authors</a></li><li><a href="#-resources">📎 Resources</a></li><li><a href="#-references">📚 References</a></li><li><a href="#-conclusion">💡 Conclusion</a></li></ul>

            </nav>
          </aside>
        
        <h2 id="-overview">🧠 Overview</h2>

<p>Modern <strong>Mixture-of-Experts (MoE)</strong> transformers activate a subset of expert functions per token, enabling large-scale models with efficient inference. These expert layers are usually assumed to be depth-specific and non-interchangeable. This project explores whether experts across layers perform <strong>functionally similar transformations</strong>, making cross-layer <strong>expert reuse</strong> feasible.</p>

<h2 id="-core-idea">🔍 Core Idea</h2>

<p>We propose a method to <strong>quantify the functional similarity</strong> between experts in different transformer layers by introducing a <strong>lightweight adapter</strong>. This adapter aligns the input distribution of one expert to another, allowing us to evaluate output similarity using <strong>Mean Squared Error (MSE)</strong>.</p>

<p>If such an adapter achieves low MSE, it implies functional similarity (up to input transformation) between the two experts.</p>

<h2 id="-problem-formulation">🧪 Problem Formulation</h2>

<p>Let:</p>

<ul>
  <li>$( f_{\ell, e} : \mathbb{R}^d \rightarrow \mathbb{R}^d )$ be the transformation implemented by expert $( e )$ in layer $( \ell )$</li>
  <li>$( D_{\ell,e} )$ be the empirical input distribution of expert $( (\ell, e) )$</li>
  <li>$( A : \mathbb{R}^d \rightarrow \mathbb{R}^d )$ be an adapter function</li>
</ul>

<p>We aim to find:</p>

<p>$[
f_{\ell_2, e_2}(A(x)) \approx f_{\ell_1, e_1}(x), \quad \forall x \sim D_{\ell_1, e_1}
]$</p>

<p>The adapter is defined as:</p>

<p>$[
A(x) = \text{LayerNorm}(Wx + b), \quad W \in \mathbb{R}^{d \times d}, b \in \mathbb{R}^d
]$</p>

<p>The training objective is:</p>

<p>$[
\mathcal{L}_{\text{MSE}} = \mathbb{E}_{x \sim D_{\ell_1,e_1}} \left[ | f_{\ell_2,e_2}(A(x)) - f_{\ell_1,e_1}(x) |_2^2 \right]
]$</p>

<h2 id="️-experimental-setup">⚙️ Experimental Setup</h2>

<ul>
  <li><strong>Model</strong>: <a href="https://huggingface.co/Qwen">Qwen1.5-MoE-A2.7B</a> (decoder-only MoE transformer)</li>
  <li><strong>Tokenization</strong>: QwenTokenizer</li>
  <li><strong>Hardware</strong>: 1x NVIDIA A100 GPU</li>
  <li><strong>Framework</strong>: HuggingFace Transformers</li>
  <li><strong>Data</strong>: Subset of English Wikipedia</li>
  <li><strong>Precision</strong>: float16</li>
  <li><strong>Adapter</strong>: Linear + LayerNorm trained to minimize MSE</li>
</ul>

<h2 id="-results">📊 Results</h2>

<h3 id="-cross-layer-expert-alignment">🔁 Cross-Layer Expert Alignment</h3>

<p><img src="https://github.com/user-attachments/assets/14f0fae5-af10-4dfe-b1d0-18ea3f786f2e" alt="activations (1)" /></p>

<p>We compare outputs of Layer 1 and Layer 2 experts using learned adapters. Example MSE alignment losses:</p>

<table>
  <thead>
    <tr>
      <th>L1→L2</th>
      <th>E0</th>
      <th>E4</th>
      <th>E8</th>
      <th>E12</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>E0</td>
      <td>8.34</td>
      <td>7.71</td>
      <td>7.81</td>
      <td>7.87</td>
    </tr>
    <tr>
      <td>E4</td>
      <td>0.017</td>
      <td>0.010</td>
      <td>0.018</td>
      <td>0.014</td>
    </tr>
    <tr>
      <td>E8</td>
      <td>0.017</td>
      <td>0.011</td>
      <td>0.016</td>
      <td>0.014</td>
    </tr>
    <tr>
      <td>E12</td>
      <td>0.015</td>
      <td>0.009</td>
      <td>0.014</td>
      <td>0.011</td>
    </tr>
  </tbody>
</table>

<p>📌 <strong>Observation</strong>: Expert 0 at Layer 1 shows consistently high MSE with all Layer 2 experts, indicating a functionally distinct behavior.</p>

<h2 id="-additional-experiment-layer-swapping">🧩 Additional Experiment: Layer Swapping</h2>

<p>To test broader functional similarity, we swapped sparse-FFN blocks across layers in the <code class="language-plaintext highlighter-rouge">Switch-Base-8</code> model and evaluated on <strong>MNLI</strong>:</p>

<table>
  <thead>
    <tr>
      <th>Swapped Layers</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1 &lt;-&gt; 3</td>
      <td>0.74</td>
    </tr>
    <tr>
      <td>1 &lt;-&gt; 5</td>
      <td>0.728</td>
    </tr>
    <tr>
      <td>1 &lt;-&gt; 7</td>
      <td>0.654</td>
    </tr>
    <tr>
      <td>1 &lt;-&gt; 9</td>
      <td>0.378</td>
    </tr>
    <tr>
      <td>1 &lt;-&gt; 11</td>
      <td>0.406</td>
    </tr>
  </tbody>
</table>

<p>📌 <strong>Observation</strong>: Functional similarity decays rapidly with layer distance.</p>

<p><img width="649" alt="token_routing_heatmap" src="https://github.com/user-attachments/assets/c44a656f-8b43-4879-a9bd-dc9752fa99cf" /></p>

<p>Swapping entire MoE blocks (router + experts) between layers confirms that closer layers tend to be more functionally aligned. Deep swaps cause routing distributions to diverge significantly.</p>

<h2 id="-future-work">🔮 Future Work</h2>

<ul>
  <li>Extend adapter-based analysis across all layer pairs</li>
  <li>Measure perplexity degradation during expert swapping</li>
  <li>Leverage functional similarity for <strong>dynamic expert routing</strong></li>
  <li>Apply results to <strong>sparsity and compression</strong></li>
  <li>Evaluate generalization to <strong>out-of-distribution</strong> data</li>
</ul>

<h2 id="-authors">👥 Authors</h2>

<ul>
  <li><a href="https://github.com/Vamsi995">Sai Vamsi Alisetti</a></li>
  <li><a href="https://github.com/Abhi12122000">Abhishek Kumar</a></li>
</ul>

<h2 id="-resources">📎 Resources</h2>

<ul>
  <li>📄 <a href="https://drive.google.com/file/d/1OdAHZ2vjIHe34CT3864xsk_MvYrVxhoh/view?usp=sharing">Project Report</a></li>
  <li>📊 <a href="https://docs.google.com/presentation/d/1JK6G4--TupMm0Y_bpSLhs93HhFvzDuwrkUw5C7wjdfM/edit?usp=sharing">Slides</a></li>
</ul>

<h2 id="-references">📚 References</h2>

<ul>
  <li>Switch Transformer: https://arxiv.org/abs/2101.03961</li>
  <li>Universal Transformers: https://arxiv.org/abs/1807.03819</li>
  <li>MoE-UT: https://arxiv.org/abs/2405.16039</li>
  <li>Expert Merging: https://arxiv.org/pdf/2310.01334</li>
</ul>

<h2 id="-conclusion">💡 Conclusion</h2>

<p>Our findings suggest that <strong>many MoE experts are functionally redundant across layers</strong>, and functional specialization is unevenly distributed. These insights pave the way for more <strong>efficient and interpretable</strong> MoE transformer architectures.</p>


        
      </section>

      <footer class="page__meta">
        
        


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2025-07-04T17:29:43-07:00">July 4, 2025</time></p>

      </footer>

      

      
  <nav class="pagination">
    
      <a href="/projects/Meetme/" class="pagination--pager" title="Meet.me
">Previous</a>
    
    
      <a href="/projects/ParaphraseGen/" class="pagination--pager" title="Paraphrase Generator with T5 transformer
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/vumsee_" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://facebook.com/alisetti.vamsi" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-facebook-square" aria-hidden="true"></i> Facebook</a></li>
        
      
        
          <li><a href="https://github.com/Vamsi995" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://instagram.com/_v.um.see_" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i> Instagram</a></li>
        
      
        
          <li><a href="https://stackoverflow.com/users/12236439/sai-vamsi" rel="nofollow noopener noreferrer"><i class="fab fa-stack-overflow" aria-hidden="true"></i> StackOverflow</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/sai-vamsi-alisetti/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="mailto:saivamsi.ds123@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        
      
    

    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 Alisetti Sai Vamsi. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







  </body>
</html>
