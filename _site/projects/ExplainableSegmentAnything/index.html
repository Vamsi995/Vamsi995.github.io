<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Explainable Grounded Segment Anything | Alisetti Sai Vamsi</title>
<meta name="description" content="An image segmentation pipeline that leverages Grounding DINO and Segment Anything Model (SAM) for text-guided image segmentation. It uses Grounding DINO as the first step to generate bounding boxes around entities in the image based on a user-provided text prompt. These bounding boxes are then passed into the Segment Anything Model (SAM), which performs finer segmentation on the image. Additionally, the pipeline features an explainable interface that uses attention maps to provide insights into the model’s decision-making process.">


  <meta name="author" content="Alisetti Sai Vamsi">
  
  <meta property="article:author" content="Alisetti Sai Vamsi">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Alisetti Sai Vamsi">
<meta property="og:title" content="Explainable Grounded Segment Anything">
<meta property="og:url" content="http://localhost:4000/projects/ExplainableSegmentAnything/">


  <meta property="og:description" content="An image segmentation pipeline that leverages Grounding DINO and Segment Anything Model (SAM) for text-guided image segmentation. It uses Grounding DINO as the first step to generate bounding boxes around entities in the image based on a user-provided text prompt. These bounding boxes are then passed into the Segment Anything Model (SAM), which performs finer segmentation on the image. Additionally, the pipeline features an explainable interface that uses attention maps to provide insights into the model’s decision-making process.">



  <meta property="og:image" content="http://localhost:4000/assets/images/projects/Explainable/explainable.png">





  <meta property="article:published_time" content="2025-01-18T22:49:31-08:00">





  

  


<link rel="canonical" href="http://localhost:4000/projects/ExplainableSegmentAnything/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Alisetti Sai Vamsi",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->




<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/landing/logo2.png" alt="Alisetti Sai Vamsi"></a>
        
        <a class="site-title" href="/">
          Alisetti Sai Vamsi
          <span class="site-subtitle">Portfolio</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/research/">Research</a>
            </li><li class="masthead__menu-item">
              <a href="/projects/">Projects</a>
            </li><li class="masthead__menu-item">
              <a href="/education/">Education</a>
            </li><li class="masthead__menu-item">
              <a href="/achievements/">Achievements</a>
            </li><li class="masthead__menu-item">
              <a href="/assets/CV/Resume.pdf/">CV</a>
            </li><li class="masthead__menu-item">
              <a href="/misc/">Misc</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      




  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/" itemprop="item"><span itemprop="name">Home</span></a>

          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">></span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/projects" itemprop="item"><span itemprop="name">Projects</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">></span>
      
    
      
      
        <li class="current">Explainable Grounded Segment Anything</li>
      
    
  </ol>
</nav>

  


<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true},
      jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
      extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
      TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
      equationNumbers: {
      autoNumber: "AMS"
      }
    }
  });
</script>

<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      <h3>GitHub</h3>
      <p><a href="https://github.com/Vamsi995/Explainable-Grounded-Segment-Anything">https://github.com/Vamsi995/Explainable-Grounded-Segment-Anything</a></p>

      
    
      
      <h3>Role</h3>
      <p>Team Lead</p>

      
    
      
      <h3>Responsibilities</h3>
      <p>Developed an image segmentation pipeline that leverages grounding DINO and segment anything model to create a text-guided image segmentation application. Utilized attention maps of transformer model to create explainable machine learning model pipeline.</p>

      
    
      
      <h3>Technology Frameworks & Languages</h3>
      <p>Python, PyTorch, Co-lab Notebooks, Numpy, Matplotlib, Gradio</p>

      
    
    
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Explainable Grounded Segment Anything">
    <meta itemprop="description" content="An image segmentation pipeline that leverages Grounding DINO and Segment Anything Model (SAM) for text-guided image segmentation. It uses Grounding DINO as the first step to generate bounding boxes around entities in the image based on a user-provided text prompt. These bounding boxes are then passed into the Segment Anything Model (SAM), which performs finer segmentation on the image. Additionally, the pipeline features an explainable interface that uses attention maps to provide insights into the model’s decision-making process.">
    <meta itemprop="datePublished" content="2025-01-18T22:49:31-08:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/projects/ExplainableSegmentAnything/" class="u-url" itemprop="url">Explainable Grounded Segment Anything
</a>
          </h1>
          


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#getting-started">Getting Started</a><ul><li><a href="#prerequisites">Prerequisites</a></li><li><a href="#installation">Installation</a></li><li><a href="#usage">Usage</a></li></ul></li><li><a href="#pipeline-flow">Pipeline Flow</a></li><li><a href="#gradio-interface">Gradio Interface</a><ul><li><a href="#panel-1-input">Panel 1: Input</a></li><li><a href="#panel-2-segmentation-output">Panel 2: Segmentation Output</a></li><li><a href="#panel-3-explanation">Panel 3: Explanation</a></li></ul></li><li><a href="#built-with">Built With</a></li><li><a href="#authors">Authors</a></li></ul>

            </nav>
          </aside>
        
        <p>An image segmentation pipeline that leverages <strong>Grounding DINO</strong> and <strong>Segment Anything Model (SAM)</strong> for text-guided image segmentation. It uses Grounding DINO as the first step to generate bounding boxes around entities in the image based on a user-provided text prompt. These bounding boxes are then passed into the Segment Anything Model (SAM), which performs finer segmentation on the image. Additionally, the pipeline features an explainable interface that uses attention maps to provide insights into the model’s decision-making process.</p>

<p><strong>Project Report</strong>: <a href="https://github.com/Vamsi995/Explainable-Grounded-Segment-Anything/blob/main/Creating_a_Visual_Explainable_AI_Interface_with_Grounded_Segment_Anything_Report.pdf">Report</a></p>

<p><img src="/assets/images/projects/Explainable/diverse.png" alt="Kids" />
<img src="/assets/images/projects/Explainable/diverse.png" alt="Diverse" /></p>

<h2 id="getting-started">Getting Started</h2>
<p>Follow the steps below to run this project locally for development and testing.</p>

<h3 id="prerequisites">Prerequisites</h3>
<p>Ensure the following libraries and frameworks are installed:</p>

<ul>
  <li>PyTorch GPU Version</li>
  <li>GroundingDINO</li>
  <li>Segment Anything</li>
</ul>

<h3 id="installation">Installation</h3>
<ul>
  <li>Clone the repository:</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/Vamsi995/Explainable-Grounded-Segment-Anything.git
<span class="nb">cd </span>Explainable-Grounded-Segment-Anything
</code></pre></div></div>

<ul>
  <li>Install Dependencies:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div>    </div>
  </li>
  <li>Grounding DINO Setup
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/IDEA-Research/GroundingDINO.git
</code></pre></div>    </div>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git checkout <span class="nt">-q</span> 57535c5a79791cb76e36fdb64975271354f10251
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip setuptools
pip <span class="nb">install </span>build wheel
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-e</span> <span class="nb">.</span>
</code></pre></div></div>

<ul>
  <li>Segment Anything Model Setup
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="s1">'git+https://github.com/facebookresearch/segment-anything.git'</span>
</code></pre></div>    </div>
  </li>
  <li>Download GroundingDINO &amp; Segment Anything Model Weights</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>weights
<span class="o">!</span>wget <span class="nt">-q</span> https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth
<span class="o">!</span>wget <span class="nt">-q</span> https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
</code></pre></div></div>

<h3 id="usage">Usage</h3>

<ul>
  <li>Run the driver script:</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python segment.py
</code></pre></div></div>

<h2 id="pipeline-flow">Pipeline Flow</h2>
<ol>
  <li><strong>Grounding DINO</strong>: The first step in the pipeline, where bounding boxes are generated based on the input text prompt. Grounding DINO identifies entities in the image that correspond to the prompt.</li>
  <li><strong>Segment Anything Model (SAM)</strong>: Once the bounding boxes are identified, SAM segments the image more precisely within these regions.</li>
  <li><strong>Explainable Interface</strong>: To offer interpretability, the pipeline extracts attention maps from the last attention layer of the Vision Transformer (ViT) block used in SAM. These maps are then overlayed on the original image as heatmaps to explain what the model is focusing on during segmentation.</li>
</ol>

<h2 id="gradio-interface">Gradio Interface</h2>
<p>The interface is simple, lightweight, and divided into three panels:</p>

<p><img src="/assets/images/projects/Explainable/Interface.png" alt="Interface" /></p>

<h3 id="panel-1-input">Panel 1: Input</h3>
<ul>
  <li><strong>Image Upload</strong>: Upload the image to be segmented.</li>
  <li><strong>Text Prompt</strong>: Input the text that specifies the entities to be segmented.</li>
  <li><strong>Segmentation Strength</strong>: Adjust the strength of segmentation to fine-tune the results.</li>
  <li><strong>Text Output</strong>: Choose whether to display the prompt text on the segmented output.</li>
  <li><strong>Run Segmentation</strong>: After inputting the image and prompt, hit the <strong>Run Segmentation</strong> button to initiate the segmentation process.</li>
</ul>

<h3 id="panel-2-segmentation-output">Panel 2: Segmentation Output</h3>
<ul>
  <li><strong>Segmented Image</strong>: Displays the segmented image after the SAM model processes the input.</li>
  <li><strong>Segmentation Masks</strong>: Provides the segmentation masks corresponding to the regions identified in the image.</li>
</ul>

<h3 id="panel-3-explanation">Panel 3: Explanation</h3>
<ul>
  <li><strong>Explain Segmentation</strong>: By clicking the <strong>Explain Segmentation</strong> button, attention maps are generated by hooking into the last attention layer of the SAM model.</li>
  <li><strong>Activation Maps</strong>: The attention maps are overlaid as heatmaps on the original image to visualize what the model is focusing on. The areas with the highest intensity represent the model’s primary focus during segmentation.</li>
</ul>

<h2 id="built-with">Built With</h2>

<ul>
  <li><a href="https://pytorch.org/">PyTorch</a> - Deep Learning Framework</li>
  <li><a href="https://www.gradio.app/">Gradio</a></li>
</ul>

<h2 id="authors">Authors</h2>
<ul>
  <li><a href="https://github.com/Vamsi995">Sai Vamsi Alisetti</a></li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2025-01-18T22:49:31-08:00">January 18, 2025</time></p>

      </footer>

      

      
  <nav class="pagination">
    
      <a href="/projects/Covariance/" class="pagination--pager" title="Covariance-Driven Graph Embedding for Real-Time Traffic Prediction
">Previous</a>
    
    
      <a href="/projects/Meetme/" class="pagination--pager" title="Meet.me
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/vumsee_" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://facebook.com/alisetti.vamsi" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-facebook-square" aria-hidden="true"></i> Facebook</a></li>
        
      
        
          <li><a href="https://github.com/Vamsi995" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://instagram.com/_v.um.see_" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i> Instagram</a></li>
        
      
        
          <li><a href="https://stackoverflow.com/users/12236439/sai-vamsi" rel="nofollow noopener noreferrer"><i class="fab fa-stack-overflow" aria-hidden="true"></i> StackOverflow</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/sai-vamsi-alisetti/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="mailto:saivamsi.ds123@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        
      
    

    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 Alisetti Sai Vamsi. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







  </body>
</html>
